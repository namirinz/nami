# -*- coding: utf-8 -*-
"""nami_ImageNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19T-Nbt1rYOZ1TJMwolxYBSzrBZv93s4S
"""

import requests
import cv2
import numpy as np
import json
import ast
from bs4 import BeautifulSoup

#!pip install cupy-cuda101
import cupy as cp

dict_nounid = {}

def get_synset_api():
  synset_path = 'https://firebasestorage.googleapis.com/v0/b/nami-5a921.appspot.com/o/synset.json?alt=media&token=70ba0c72-24e2-4cb1-b379-0a332892eb78'
  with requests.Session() as req:
    resp = req.get(synset_path)
    dict_nounid = ast.literal_eval(resp.content.decode('utf-8'))
    return dict_nounid

def get_synset_gdrive(synset_path):
  with open(synset_path) as file:
    dict_nounid = json.load(file)
    return dict_nounid

dict_nounid = get_synset_api()

def get_nounids(noun):
  '''
  Check if nounid avliable

  Parameters:
    noun (string): name of image dataset you want.

  Returns:
    - array of nounid if avaliable.
    - raise KeyError if noun is not avaliable.
  '''
  try:
    dict_nounid[noun]
    return dict_nounid[noun]
  except KeyError:
    raise KeyError("Error",noun,"is not in the synset or maybe unavaliable...")
  
def request_img(url, dimension, timeout):
  with requests.Session() as req:
    resp = req.get(url, stream=True, timeout=timeout)

  image = np.asarray(bytearray(resp.content), dtype='uint8')
  image = cv2.imdecode(image, cv2.IMREAD_COLOR)
    
  image = cv2.resize(image, dsize=dimension)
  image_cupy = cp.asarray(image)

  return image_cupy

def get_dataset(noun, dimension=(224,224), max=None, timeout=0.5, save_to=None):
  """
  Get the noun image datasets from ImageNet.

  Parameters:
    noun (string) : aka.(nounid) what image dataset to get.
    dimension (tuple of int) : size of image.
    max (int) : number of images.
    timeout (float) : maxmium timeout for http request.
    save_to (str) : 'path/filename.npy' file name to save

  """
  mod = [0.2*i*max for i in range(1,6)]
  try:
    noun_ids = get_nounids(noun)
  except KeyError:
   raise KeyError("Error",noun,"is not in the synset or maybe unavaliable...")
  
  is_err = False
  correct = 0
  dataset = cp.asarray([])

  for noun_id in noun_ids:
    if max != None and dataset.shape[0] == max:
      break

    print('nounid:',noun_id,":",dataset.shape[0], "images...")
    
    small_dataset = cp.asarray([])
    image_url = "http://www.image-net.org/api/text/imagenet.synset.geturls?wnid="+str(noun_id)
    
    try:
      with requests.Session() as req:
        page = req.get(image_url)
    except TimeoutError:
      continue

    soup = str(BeautifulSoup(page.content, 'html.parser'))
    split_url = soup.split()
    length = len(split_url)
  
    for index, item in enumerate(split_url):
      if correct in mod and not is_err:
        print(index+1,'urls from',length)
        print(correct,'images have been generated...')
        
      if max != None and dataset.shape[0] + small_dataset.shape[0] == max:
        break

      # use `try` because some requested url are not avaliable
      try:
        image_cupy = request_img(url = item, dimension=dimension, timeout=timeout)
        image_cupy = cp.reshape(image_cupy, (1, dimension[0], dimension[1], 3))
          
        correct += 1        
          
        if small_dataset.shape[0] == 0:
          small_dataset = cp.asarray(image_cupy)
        else :
          small_dataset = cp.concatenate((small_dataset,image_cupy), axis=0)

        is_err = False

      except:
        is_err = True
        continue

    if noun_id == noun_ids[0]:
      dataset = cp.asarray(small_dataset)
    else:
      dataset = cp.concatenate((dataset, small_dataset), axis=0)
  print('-'*30)
  print('load',dataset.shape[0],'images dataset of',noun,'successful...')
  print('Save to path:',save_to)
  cp.save(save_to, dataset)